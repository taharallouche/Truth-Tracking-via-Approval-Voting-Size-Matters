{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from size_matters.aggregation.aggregators import (\n",
    "    Aggregator,\n",
    "    StandardApprovalAggregator,\n",
    "    CondorcetAggregator,\n",
    "    EuclidAggregator,\n",
    "    DiceAggregator,\n",
    "    JaccardAggregator,\n",
    ")\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from size_matters.utils.inventory import COLUMNS\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PATH = Path(\"../data/countries\")\n",
    "ALTERNATIVES = [\"France\", \"Tunisia\", \"Egypt\", \"Greece\", \"Spain\"]\n",
    "EXPORT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(FOLDER_PATH / \"raw.csv\")\n",
    "raw = raw.drop(columns=[\"Horodateur\", \"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth = raw[raw[\"Choose you pseudo\"] == \"GroudTruth\"].drop(\n",
    "    columns=[\"Choose you pseudo\"]\n",
    ")\n",
    "annotations = raw[raw[\"Choose you pseudo\"] != \"GroudTruth\"].drop(\n",
    "    columns=[\"Choose you pseudo\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth = groundtruth.T\n",
    "groundtruth[0] = pd.Categorical(groundtruth[0])\n",
    "groundtruth = pd.get_dummies(groundtruth[0])\n",
    "groundtruth.index.name = COLUMNS.question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT:\n",
    "    groundtruth.to_csv(FOLDER_PATH / \"groundtruth.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = annotations.T\n",
    "annotations = annotations.stack()\n",
    "annotations.index.names = [COLUMNS.question, COLUMNS.voter]\n",
    "\n",
    "multi_answers = annotations.str.split(\", \", expand=False)\n",
    "annotations = pd.DataFrame(index=annotations.index, columns=ALTERNATIVES)\n",
    "annotations = annotations.apply(\n",
    "    lambda x: multi_answers.apply(lambda y: int(x.name in y))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT:\n",
    "    annotations.to_csv(FOLDER_PATH / \"annotations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate and compare performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATORS: list[Aggregator] = [\n",
    "    StandardApprovalAggregator(),\n",
    "    CondorcetAggregator(),\n",
    "    EuclidAggregator(),\n",
    "    DiceAggregator(),\n",
    "    JaccardAggregator(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = pd.Series(\n",
    "    {\n",
    "        aggregator.type: accuracy_score(groundtruth, aggregator.aggregate(annotations))\n",
    "        for aggregator in AGGREGATORS\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crowd-label-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
