{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from size_matters.aggregation.aggregators import apply_condorcet_aggregator, apply_mallow_aggregator, apply_standard_approval_aggregator\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from size_matters.utils.inventory import COLUMNS, RULES\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PATH = Path(\"../data/countries\")\n",
    "ALTERNATIVES = [\"France\", \"Tunisia\", \"Egypt\", \"Greece\", \"Spain\"]\n",
    "EXPORT = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(FOLDER_PATH / \"raw.csv\")\n",
    "raw = raw.drop(columns=[\"Horodateur\",\"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth = raw[raw[\"Choose you pseudo\"]==\"GroudTruth\"].drop(columns=[\"Choose you pseudo\"])\n",
    "annotations = raw[raw[\"Choose you pseudo\"]!=\"GroudTruth\"].drop(columns=[\"Choose you pseudo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth = groundtruth.T\n",
    "groundtruth[0] = pd.Categorical(groundtruth[0])\n",
    "groundtruth = pd.get_dummies(groundtruth[0])\n",
    "groundtruth.index.name = COLUMNS.question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT:\n",
    "    groundtruth.to_csv(FOLDER_PATH / \"groundtruth.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = annotations.T\n",
    "annotations = annotations.stack()\n",
    "annotations.index.names = [COLUMNS.question, COLUMNS.voter]\n",
    "\n",
    "multi_answers  = annotations.str.split(\", \",expand=False)\n",
    "annotations = pd.DataFrame(index=annotations.index, columns=ALTERNATIVES)\n",
    "annotations = annotations.apply(lambda x: multi_answers.apply(lambda y: int(x.name in y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT:\n",
    "    annotations.to_csv(FOLDER_PATH / \"annotations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate and compare performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregations =  {\"standard_approval\": apply_standard_approval_aggregator(annotations),\"condorcet\": apply_condorcet_aggregator(annotations),\n",
    "                 \"jaccard\": apply_mallow_aggregator(annotations, RULES.jaccard), \"euclid\": apply_mallow_aggregator(annotations, RULES.euclid),\n",
    "                 \"dice\": apply_mallow_aggregator(annotations, RULES.dice)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = pd.Series({rule: accuracy_score(groundtruth, aggregated) for rule, aggregated in aggregations.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crowd-label-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
